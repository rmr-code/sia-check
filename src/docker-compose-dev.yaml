services:
  # this downloads the model once on to host machine and exits
  model-downloader:
    build: 
      context: ./model-downloader
    image: rmrhub/model-downloader:v0.1.0
    volumes:
      - ./${DATA_DIR}:/app/${DATA_DIR}  
    env_file:
      - .env
  
  # llm-server:
  #   image: vllm/vllm-openai:latest
  #   ports:
  #     - "8000:8000"
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   env_file:
  #     - .env
  #   ipc: "host"
  #   runtime: nvidia
  #   command: ["--model", "${LLM_MODEL_NAME}", "--dtype", "${DTYPE}"]

  # this embeddings-server is called from the api to generate embeddings
  embeddings-server:
    build: 
      context: ./embeddings-server
      dockerfile: Dockerfile
    image: rmrhub/sia-embeddings-server:v0.1.0
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env  # Use environment variables from .env file
    volumes:
      - ./${DATA_DIR}:/app/${DATA_DIR}  # Mount shared data directory
    depends_on:
      - model-downloader
    ports:
      - "8002:8002"  # Expose port for the embeddings server API    
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # this is the main api-server accessed by the web-server    
  api-server:
    build:
      context: ./api-server
      dockerfile: Dockerfile
    hostname: api-server # required for web-server to proxy_pass
    image: rmrhub/sia-api-server:v0.1.0
    environment:
      - PYTHONUNBUFFERED=1 # can be removed in production
    env_file:
      - .env
    ports:
      - "8080:8080"
    depends_on:
      - embeddings-server
    volumes:
      - ./${DATA_DIR}:/app/${DATA_DIR}:rw  # Shared data directory
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    command: >
      sh -c "chmod -R 777 /app/${DATA_DIR} && uvicorn main:app --host 0.0.0.0 --port 8080 --workers ${API_NO_WORKERS:-1}"
  
  # this web-server is the interface for the admin and chat user
  web-server:
    build:
      context: ./web-server
      dockerfile: Dockerfile
    image: rmrhub/sia-web-server:v0.1.1
    ports:
      - "3000:3000"
      # Uncomment the following line to expose HTTPS port
      # - "443:443"
    depends_on:
      - api-server
    volumes:
      - ./web-server/nginx.conf:/etc/nginx/nginx.conf:ro
