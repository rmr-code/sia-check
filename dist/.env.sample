# ------------ variables used by API-SERVER
#
# no of workers
API_NO_WORKERS=4
# used for JWT Tokens
SECRET_KEY=6P9iBm9a4cBBAAFUSqyQ7Ucz2C9oafrc
# The application will automatically create subdirectories
# called AGENTS, STORE and MODELS. Ensure the chmod is 777
DATA_DIR=data
# Duration of cookie in hours
TOKEN_EXPIRY_IN_HOURS=24
# embeddings-server details
EMBEDDINGS_SERVER=embeddings-server #name of the service
EMBEDDINGS_SERVER_PORT=8002 # port used by the embeddings-server
# llm-server details
LLM_SERVER=llm-server #name of the service
LLM_SERVER_PORT=8000 # port used by the llm-server
# Chat Request Parameters
# Response Length
CHAT_RESPONSE_LENGTH_DEFAULT=M # Medium, Short, Long
# Below are the thresholds for above abbr
CHAT_RESPONSE_LENGTH_SHORT=50,   # Short response tokens
CHAT_RESPONSE_LENGTH_MEDIUM=150 # Medium response tokens
CHAT_RESPONSE_LENGTH_LONG=300 # Long response tokesn
# Temperature for randomness in responses
CHAT_TEMPERATURE=0.7  # 0.0 (deterministic) to 1.0 (more creative)
# Maximum number of tokens to generate in a response
CHAT_MAX_TOKENS=200  # Adjust based on response length preference
# Top-p sampling parameter (nucleus sampling)
CHAT_TOP_P=0.9  # A higher value will allow more randomness
# Frequency penalty (reduces likelihood of repetitive words)
CHAT_FREQUENCY_PENALTY=0.0  # Values from 0.0 to 2.0
# Presence penalty (increases likelihood of introducing new concepts)
CHAT_PRESENCE_PENALTY=0.0  # Values from 0.0 to 2.0

# ------------ variables used by EMBEDDINGS-SERVER
#
# no of workers
EMBEDDINGS_NO_WORKERS=1
# model name of sentence-transformers type
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# pytorch_model.bin is the usual filename but kept it in .env for flexibility
EMBEDDING_MODEL_FILENAME=pytorch_model.bin 
# Hugging Face token. Some models may require this
HF_API_TOKEN=<PUT-YOUR-HF-TOKEN>
# api-server details
API_SERVER=api-server #name of the service
API_SERVER_PORT=8080 # port used by the api-server

# ------------ variables used by used by LLM-SERVER
#
# Model name to use (LLM_MODEL_NAME is required)
LLM_MODEL_NAME=microsoft/Phi-3-mini-4k-instruct

# Data type for the model weights. 
# Options: "auto", "float16", "float32" (default)
DTYPE=float16  # Use float16 for faster inference with lower memory usage (recommended if GPU supports it)

# Number of workers for inference. Default is 1.
LLM_NUM_WORKERS=4  # Adjust based on the available CPU cores and the number of parallel requests expected

# Fraction of GPU memory to use. Float between 0 and 1.
VLLM_GPU_MEMORY_FRACTION=0.9  # Use 90% of GPU memory. Adjust based on available memory.

# Number of transformer layers to keep on GPU (if using offloading to CPU).
# Keep this lower for models that are too large to fit fully in GPU memory.
VLLM_NUM_GPU_LAYERS=40  # Set based on GPU memory. Increase if you want to keep more layers in GPU memory.

# Maximum sequence length in tokens. Default depends on the model (e.g., 2048 for GPT models).
VLLM_MAX_SEQ_LENGTH=4096  # Set based on the model's maximum sequence length. Recommended to match the model's capabilities.

# Enable offloading layers to CPU if GPU memory is insufficient.
VLLM_OFFLOAD_CPU=true  # Useful when GPU memory is low and model layers need to be offloaded to CPU.

# since models are loaded locally an OPEN API key is not required
# however in case of using it with Open AI's own model it is required
OPENAPI_KEY=None
