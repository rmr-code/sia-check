services:
  # this downloads the model once on to host machine and exits
  model_downloader: 
    image: amikos/hf-model-downloader
    env_file:
      - .env  # Use environment variables from .env file
    command: ${EMBEDDING_MODEL_NAME}
    volumes:
      - ./${DATA_DIR}/models:/models
    environment:
      - USE_CACHE=TRUE

  llm-server:
     image: vllm/vllm-openai:latest
     ports:
       - "8000:8000"
     volumes:
       - ~/.cache/huggingface:/root/.cache/huggingface
     env_file:
       - .env
     ipc: "host"
     # comment if used in only cpu server environment. 
     deploy:
       resources:
         reservations:
           devices:
             - capabilities: [gpu]
     runtime: nvidia
     command: ["--model", "${LLM_MODEL_NAME}", "--dtype", "${DTYPE}"]

  embeddings-server:
    image: rmrhub/sia-embeddings-server:v0.1.1
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env  # Use environment variables from .env file
    volumes:
      - ./${DATA_DIR}:/data  # Mount shared data directory
    depends_on:
      - model_downloader
    ports:
      - "8002:8002"  # Expose port for the embeddings server API    

  api-server:
    image: rmrhub/sia-api-server:v0.1.1
    hostname: api-server # required for web-server to proxy_pass
    environment:
      - PYTHONUNBUFFERED=1 # can be removed in production
    env_file:
      - .env
    ports:
      - 8080:8080
    depends_on:
      - embeddings-server
    volumes:
      - ./${DATA_DIR}:/app/data  # Shared data directory

  web-server:
    image: rmrhub/sia-web-server:v0.1.1
    ports:
      - "80:80"
      # Uncomment the following line to expose HTTPS port
      # - "443:443"
    depends_on:
      - api-server
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      # Mount the SSL certificates directory (uncomment if using HTTPS)
      # - ./ssl:/etc/nginx/ssl:ro